"""
Module Early Warning System - H·ªá th·ªëng C·∫£nh b√°o R·ªßi ro S·ªõm
S·ª≠ d·ª•ng ML (Stacking + K-Means + Gemini AI) ƒë·ªÉ ch·∫©n ƒëo√°n s·ª©c kh·ªèe t√†i ch√≠nh doanh nghi·ªáp
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import os


class EarlyWarningSystem:
    """
    H·ªá th·ªëng C·∫£nh b√°o R·ªßi ro S·ªõm (Early Warning System)

    Ch·ª©c nƒÉng ch√≠nh:
    1. Train Stacking model (RF + XGB + GB) + K-Means clustering
    2. T√≠nh Health Score (0-100) d·ª±a tr√™n 14 ch·ªâ s·ªë v√† feature importances
    3. Ph√¢n lo·∫°i m·ª©c r·ªßi ro (Safe/Watch/Warning/Alert)
    4. Ph√°t hi·ªán ƒëi·ªÉm y·∫øu (top 3 ch·ªâ s·ªë xa ng∆∞·ª°ng an to√†n nh·∫•t)
    5. X√°c ƒë·ªãnh v·ªã tr√≠ trong cluster
    6. D·ª± b√°o PD trong t∆∞∆°ng lai (3/6/12 th√°ng) theo k·ªãch b·∫£n vƒ© m√¥
    7. T·∫°o b√°o c√°o ch·∫©n ƒëo√°n b·∫±ng Gemini AI
    """

    def __init__(self):
        """Kh·ªüi t·∫°o Early Warning System"""
        self.stacking_model = None
        self.kmeans = None
        self.scaler = StandardScaler()
        self.thresholds = {}  # Ng∆∞·ª°ng an to√†n cho 14 ch·ªâ s·ªë
        self.feature_importances = {}
        self.training_data = None
        self.cluster_info = {}

        # T√™n ƒë·∫ßy ƒë·ªß c·ªßa 14 ch·ªâ s·ªë
        self.indicator_names = {
            'X_1': 'Bi√™n l·ª£i nhu·∫≠n g·ªôp',
            'X_2': 'Bi√™n l·ª£i nhu·∫≠n tr∆∞·ªõc thu·∫ø',
            'X_3': 'ROA (L·ª£i nhu·∫≠n/T√†i s·∫£n)',
            'X_4': 'ROE (L·ª£i nhu·∫≠n/VCSH)',
            'X_5': 'N·ª£/T√†i s·∫£n',
            'X_6': 'N·ª£/V·ªën ch·ªß s·ªü h·ªØu',
            'X_7': 'Thanh to√°n hi·ªán h√†nh',
            'X_8': 'Thanh to√°n nhanh',
            'X_9': 'Kh·∫£ nƒÉng tr·∫£ l√£i',
            'X_10': 'Kh·∫£ nƒÉng tr·∫£ n·ª£ g·ªëc',
            'X_11': 'T·∫°o ti·ªÅn/VCSH',
            'X_12': 'V√≤ng quay h√†ng t·ªìn kho',
            'X_13': 'K·ª≥ thu ti·ªÅn b√¨nh qu√¢n',
            'X_14': 'Hi·ªáu su·∫•t s·ª≠ d·ª•ng t√†i s·∫£n'
        }

    def train_models(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Train Stacking model v√† K-Means clustering

        Args:
            df: DataFrame ch·ª©a 14 ch·ªâ s·ªë (X_1 ‚Üí X_14) + c·ªôt 'label' (0=kh√¥ng v·ª° n·ª£, 1=v·ª° n·ª£)

        Returns:
            Dict ch·ª©a th√¥ng tin v·ªÅ training:
            - num_samples: S·ªë l∆∞·ª£ng m·∫´u
            - feature_importances: Feature importances t·ª´ RandomForest
            - cluster_distribution: Ph√¢n b·ªë c√°c cluster
        """
        print("üîÑ B·∫Øt ƒë·∫ßu train Early Warning System...")

        # L∆∞u training data
        self.training_data = df.copy()

        # T√°ch features v√† labels
        feature_cols = [f'X_{i}' for i in range(1, 15)]
        X = df[feature_cols].values
        y = df['label'].values

        # 1. TRAIN STACKING MODEL (RF + XGB + GB, meta=LogisticRegression)
        print("üìä Training Stacking Classifier...")

        # Base models
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )

        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            n_jobs=-1
        )

        gb_model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )

        # Meta model
        meta_model = LogisticRegression(max_iter=1000)

        # Stacking
        self.stacking_model = StackingClassifier(
            estimators=[
                ('rf', rf_model),
                ('xgb', xgb_model),
                ('gb', gb_model)
            ],
            final_estimator=meta_model,
            cv=5
        )

        self.stacking_model.fit(X, y)
        print("‚úÖ Stacking model trained!")

        # Extract feature importances t·ª´ RandomForest layer
        rf_estimator = self.stacking_model.named_estimators_['rf']
        importances = rf_estimator.feature_importances_

        self.feature_importances = {
            feature_cols[i]: float(importances[i])
            for i in range(len(feature_cols))
        }

        print("üìà Feature Importances:")
        for feature, importance in sorted(self.feature_importances.items(), key=lambda x: x[1], reverse=True):
            print(f"   {feature}: {importance:.4f}")

        # 2. TRAIN K-MEANS CLUSTERING (4 clusters)
        print("üîç Training K-Means (4 clusters)...")

        # Ch·ªâ cluster nh√≥m kh√¥ng v·ª° n·ª£ (label=0)
        X_healthy = df[df['label'] == 0][feature_cols].values

        self.kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        self.kmeans.fit(X_healthy)

        cluster_labels = self.kmeans.predict(X_healthy)

        # T√≠nh th√¥ng tin cluster
        for cluster_id in range(4):
            cluster_mask = cluster_labels == cluster_id
            cluster_data = X_healthy[cluster_mask]

            self.cluster_info[cluster_id] = {
                'size': int(np.sum(cluster_mask)),
                'center': self.kmeans.cluster_centers_[cluster_id].tolist(),
                'avg_values': np.mean(cluster_data, axis=0).tolist()
            }

        print("‚úÖ K-Means trained!")
        print(f"   Cluster sizes: {[self.cluster_info[i]['size'] for i in range(4)]}")

        # 3. T√çNH NG∆Ø·ª†NG AN TO√ÄN (percentile P40, P50, P60 c·ªßa nh√≥m label=0)
        print("üìè Calculating safety thresholds...")

        df_healthy = df[df['label'] == 0]

        for col in feature_cols:
            # M·ªôt s·ªë ch·ªâ s·ªë c√†ng cao c√†ng t·ªët (sinh l·ªùi, thanh to√°n)
            # M·ªôt s·ªë ch·ªâ s·ªë c√†ng th·∫•p c√†ng t·ªët (n·ª£, k·ª≥ thu ti·ªÅn)

            # Ch·ªâ s·ªë c√†ng cao c√†ng t·ªët: X_1, X_2, X_3, X_4, X_7, X_8, X_9, X_10, X_11, X_12, X_14
            # Ch·ªâ s·ªë c√†ng th·∫•p c√†ng t·ªët: X_5, X_6, X_13

            if col in ['X_5', 'X_6', 'X_13']:
                # C√†ng th·∫•p c√†ng t·ªët ‚Üí ng∆∞·ª°ng an to√†n l√† P60 (kh√¥ng v∆∞·ª£t qu√°)
                self.thresholds[col] = {
                    'safe_zone': float(df_healthy[col].quantile(0.40)),
                    'watch_zone': float(df_healthy[col].quantile(0.50)),
                    'warning_zone': float(df_healthy[col].quantile(0.60)),
                    'direction': 'lower_is_better'
                }
            else:
                # C√†ng cao c√†ng t·ªët ‚Üí ng∆∞·ª°ng an to√†n l√† P40 (kh√¥ng th·∫•p h∆°n)
                self.thresholds[col] = {
                    'safe_zone': float(df_healthy[col].quantile(0.60)),
                    'watch_zone': float(df_healthy[col].quantile(0.50)),
                    'warning_zone': float(df_healthy[col].quantile(0.40)),
                    'direction': 'higher_is_better'
                }

        print("‚úÖ Thresholds calculated!")

        # 4. Tr·∫£ v·ªÅ th√¥ng tin training
        result = {
            'num_samples': len(df),
            'num_healthy': int(np.sum(df['label'] == 0)),
            'num_default': int(np.sum(df['label'] == 1)),
            'feature_importances': self.feature_importances,
            'cluster_distribution': {
                f'cluster_{i}': self.cluster_info[i]['size']
                for i in range(4)
            }
        }

        print("‚úÖ Early Warning System trained successfully!")
        return result

    def calculate_health_score(self, indicators: Dict[str, float]) -> float:
        """
        T√≠nh Health Score (0-100) d·ª±a tr√™n 60% PD + 40% Statistical

        Args:
            indicators: Dict ch·ª©a 14 ch·ªâ s·ªë (X_1 ‚Üí X_14)

        Returns:
            Health Score (0-100)

        C√¥ng th·ª©c:
            1. T√≠nh Statistical Score d·ª±a tr√™n thresholds v√† feature importances
            2. T√≠nh PD Score t·ª´ stacking_model
            3. Health Score = 60% * (100 - PD) + 40% * Statistical Score
        """
        if not self.feature_importances:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train. Vui l√≤ng g·ªçi train_models() tr∆∞·ªõc.")

        if self.stacking_model is None:
            raise ValueError("Stacking model ch∆∞a ƒë∆∞·ª£c train. Vui l√≤ng g·ªçi train_models() tr∆∞·ªõc.")

        # 1. T√çNH STATISTICAL SCORE (40%)
        total_score = 0.0
        total_weight = 0.0

        for indicator, value in indicators.items():
            if indicator not in self.thresholds:
                continue

            threshold_info = self.thresholds[indicator]
            importance = self.feature_importances.get(indicator, 0.0)

            # Normalize v·ªÅ [0, 1]
            if threshold_info['direction'] == 'higher_is_better':
                # C√†ng cao c√†ng t·ªët
                safe = threshold_info['safe_zone']
                warning = threshold_info['warning_zone']

                if value >= safe:
                    normalized = 1.0
                elif value <= warning:
                    normalized = 0.0
                else:
                    normalized = (value - warning) / (safe - warning) if safe != warning else 0.5
            else:
                # C√†ng th·∫•p c√†ng t·ªët
                safe = threshold_info['safe_zone']
                warning = threshold_info['warning_zone']

                if value <= safe:
                    normalized = 1.0
                elif value >= warning:
                    normalized = 0.0
                else:
                    normalized = (warning - value) / (warning - safe) if warning != safe else 0.5

            # Weighted sum
            total_score += normalized * importance
            total_weight += importance

        # Statistical score (0-100)
        statistical_score = (total_score / total_weight * 100) if total_weight > 0 else 50.0
        statistical_score = max(0.0, min(100.0, statistical_score))

        # 2. T√çNH PD SCORE (60%)
        feature_cols = [f'X_{i}' for i in range(1, 15)]
        X_input = [[indicators[col] for col in feature_cols]]
        pd_value = self.stacking_model.predict_proba(X_input)[0, 1] * 100  # PD in %

        # PD Score: 100 - PD (PD c√†ng th·∫•p ‚Üí score c√†ng cao)
        pd_score = max(0.0, min(100.0, 100 - pd_value))

        # 3. K·∫æT H·ª¢P: 60% PD + 40% Statistical
        health_score = 0.6 * pd_score + 0.4 * statistical_score

        # Gi·ªõi h·∫°n trong [0, 100]
        health_score = max(0.0, min(100.0, health_score))

        return round(health_score, 2)

    def classify_risk_level(self, health_score: float) -> Dict[str, str]:
        """
        Ph√¢n lo·∫°i m·ª©c r·ªßi ro d·ª±a tr√™n Health Score

        Args:
            health_score: Health Score (0-100)

        Returns:
            Dict ch·ª©a risk_level v√† risk_level_color
        """
        if health_score >= 80:
            return {
                'risk_level': 'Safe',
                'risk_level_color': '#10B981',
                'risk_level_icon': 'üü¢',
                'risk_level_text': 'An to√†n'
            }
        elif health_score >= 60:
            return {
                'risk_level': 'Watch',
                'risk_level_color': '#F59E0B',
                'risk_level_icon': 'üü°',
                'risk_level_text': 'Theo d√µi'
            }
        elif health_score >= 40:
            return {
                'risk_level': 'Warning',
                'risk_level_color': '#FF8C00',
                'risk_level_icon': 'üü†',
                'risk_level_text': 'C·∫£nh b√°o'
            }
        else:
            return {
                'risk_level': 'Alert',
                'risk_level_color': '#EF4444',
                'risk_level_icon': 'üî¥',
                'risk_level_text': 'Nguy hi·ªÉm'
            }

    def detect_weaknesses(self, indicators: Dict[str, float]) -> List[Dict[str, Any]]:
        """
        Ph√°t hi·ªán ƒëi·ªÉm y·∫øu (top 3 ch·ªâ s·ªë xa ng∆∞·ª°ng an to√†n nh·∫•t)

        Args:
            indicators: Dict ch·ª©a 14 ch·ªâ s·ªë

        Returns:
            List top 3 ch·ªâ s·ªë y·∫øu nh·∫•t
        """
        weaknesses = []

        for indicator, value in indicators.items():
            if indicator not in self.thresholds:
                continue

            threshold_info = self.thresholds[indicator]
            safe_threshold = threshold_info['safe_zone']
            direction = threshold_info['direction']

            # T√≠nh gap (kho·∫£ng c√°ch so v·ªõi ng∆∞·ª°ng an to√†n)
            if direction == 'higher_is_better':
                gap = value - safe_threshold
                severity = 'critical' if gap < -safe_threshold * 0.3 else 'moderate' if gap < 0 else 'low'
            else:
                gap = safe_threshold - value
                severity = 'critical' if gap < -safe_threshold * 0.3 else 'moderate' if gap < 0 else 'low'

            # T√≠nh percentile
            if self.training_data is not None:
                healthy_data = self.training_data[self.training_data['label'] == 0]
                if indicator in healthy_data.columns:
                    percentile = (healthy_data[indicator] < value).sum() / len(healthy_data) * 100
                else:
                    percentile = 50.0
            else:
                percentile = 50.0

            weaknesses.append({
                'indicator': indicator,
                'name': self.indicator_names.get(indicator, indicator),
                'current_value': round(value, 4),
                'safe_threshold': round(safe_threshold, 4),
                'gap': round(gap, 4),
                'percentile': round(percentile, 1),
                'severity': severity,
                'direction': direction
            })

        # S·∫Øp x·∫øp theo gap (√¢m nh·∫•t = y·∫øu nh·∫•t)
        weaknesses.sort(key=lambda x: x['gap'])

        # Tr·∫£ v·ªÅ top 3
        return weaknesses[:3]

    def get_cluster_position(self, indicators: Dict[str, float]) -> Dict[str, Any]:
        """
        X√°c ƒë·ªãnh v·ªã tr√≠ DN trong cluster

        Args:
            indicators: Dict ch·ª©a 14 ch·ªâ s·ªë

        Returns:
            Dict ch·ª©a cluster_id, cluster_name, position_percentile, cluster_avg_pd
        """
        if self.kmeans is None:
            raise ValueError("K-Means ch∆∞a ƒë∆∞·ª£c train. Vui l√≤ng g·ªçi train_models() tr∆∞·ªõc.")

        # Chu·∫©n b·ªã input
        feature_cols = [f'X_{i}' for i in range(1, 15)]
        X_input = np.array([[indicators[col] for col in feature_cols]])

        # Predict cluster
        cluster_id = int(self.kmeans.predict(X_input)[0])

        # T√≠nh percentile trong to√†n dataset
        if self.training_data is not None:
            healthy_data = self.training_data[self.training_data['label'] == 0]

            # T√≠nh kho·∫£ng c√°ch ƒë·∫øn center c·ªßa cluster
            center = self.kmeans.cluster_centers_[cluster_id]
            distances = np.linalg.norm(healthy_data[feature_cols].values - center, axis=1)
            current_distance = np.linalg.norm(X_input[0] - center)

            # Percentile: v·ªã tr√≠ c·ªßa DN trong to√†n b·ªô healthy dataset
            # T√≠nh d·ª±a tr√™n health score
            health_scores = []
            for _, row in healthy_data.iterrows():
                row_indicators = {col: row[col] for col in feature_cols}
                hs = self.calculate_health_score(row_indicators)
                health_scores.append(hs)

            current_health_score = self.calculate_health_score(indicators)
            position_percentile = (np.array(health_scores) < current_health_score).sum() / len(health_scores) * 100
        else:
            position_percentile = 50.0

        # T√™n cluster (d·ª±a tr√™n percentile)
        if position_percentile >= 75:
            cluster_name = "üü¢ Nh√≥m A - Xu·∫•t s·∫Øc"
        elif position_percentile >= 50:
            cluster_name = "üü° Nh√≥m B - T·ªët"
        elif position_percentile >= 25:
            cluster_name = "üü† Nh√≥m C - Y·∫øu"
        else:
            cluster_name = "üî¥ Nh√≥m D - R·∫•t y·∫øu"

        # T√≠nh cluster avg PD (n·∫øu c√≥ stacking model)
        cluster_avg_pd = 0.0
        if self.stacking_model is not None and self.training_data is not None:
            # L·∫•y t·∫•t c·∫£ DN trong cluster n√†y
            cluster_mask = self.kmeans.predict(healthy_data[feature_cols].values) == cluster_id
            cluster_data = healthy_data[cluster_mask]

            if len(cluster_data) > 0:
                # D·ª± b√°o PD cho cluster
                X_cluster = cluster_data[feature_cols].values
                cluster_pds = self.stacking_model.predict_proba(X_cluster)[:, 1] * 100
                cluster_avg_pd = float(np.mean(cluster_pds))

        # T√≠nh median indicators c·ªßa cluster
        if self.training_data is not None:
            healthy_data = self.training_data[self.training_data['label'] == 0]
            cluster_mask = self.kmeans.predict(healthy_data[feature_cols].values) == cluster_id
            cluster_data = healthy_data[cluster_mask]

            cluster_median_indicators = {}
            for col in feature_cols:
                if col in cluster_data.columns:
                    cluster_median_indicators[col] = float(cluster_data[col].median())
                else:
                    cluster_median_indicators[col] = 0.0
        else:
            cluster_median_indicators = {col: 0.0 for col in feature_cols}

        return {
            'cluster_id': cluster_id,
            'cluster_name': cluster_name,
            'position_percentile': round(position_percentile, 1),
            'cluster_avg_pd': round(cluster_avg_pd, 2),
            'cluster_median_indicators': cluster_median_indicators
        }

    def project_future_pd(
        self,
        indicators: Dict[str, float],
        months: int,
        scenario: str,
        excel_processor,
        industry_code: str = "manufacturing"
    ) -> float:
        """
        D·ª± b√°o PD trong t∆∞∆°ng lai theo k·ªãch b·∫£n vƒ© m√¥

        Args:
            indicators: Dict 14 ch·ªâ s·ªë hi·ªán t·∫°i
            months: S·ªë th√°ng d·ª± b√°o (3/6/12)
            scenario: K·ªãch b·∫£n ("recession_mild", "recession_moderate", "crisis")
            excel_processor: Instance c·ªßa ExcelProcessor
            industry_code: M√£ ng√†nh

        Returns:
            PD d·ª± b√°o (%)
        """
        if self.stacking_model is None:
            raise ValueError("Stacking model ch∆∞a ƒë∆∞·ª£c train. Vui l√≤ng g·ªçi train_models() tr∆∞·ªõc.")

        # K·ªãch b·∫£n vƒ© m√¥
        macro_scenarios = {
            'recession_mild': {
                'gdp_growth_pct': -1.5,
                'inflation_cpi_pct': 6.0,
                'inflation_ppi_pct': 8.0,
                'policy_rate_change_bps': 100,
                'fx_usd_vnd_pct': 3.0
            },
            'recession_moderate': {
                'gdp_growth_pct': -3.5,
                'inflation_cpi_pct': 10.0,
                'inflation_ppi_pct': 14.0,
                'policy_rate_change_bps': 200,
                'fx_usd_vnd_pct': 6.0
            },
            'crisis': {
                'gdp_growth_pct': -6.0,
                'inflation_cpi_pct': 15.0,
                'inflation_ppi_pct': 20.0,
                'policy_rate_change_bps': 300,
                'fx_usd_vnd_pct': 10.0
            }
        }

        if scenario not in macro_scenarios:
            scenario = 'recession_mild'

        macro_vars = macro_scenarios[scenario]

        # K√™nh truy·ªÅn d·∫´n macro ‚Üí micro
        micro_shocks = excel_processor.macro_to_micro_transmission(
            gdp_growth_pct=macro_vars['gdp_growth_pct'],
            inflation_cpi_pct=macro_vars['inflation_cpi_pct'],
            inflation_ppi_pct=macro_vars['inflation_ppi_pct'],
            policy_rate_change_bps=macro_vars['policy_rate_change_bps'],
            fx_usd_vnd_pct=macro_vars['fx_usd_vnd_pct'],
            industry_code=industry_code
        )

        # ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô shock theo s·ªë th√°ng (c√†ng xa c√†ng m·∫°nh)
        time_multiplier = months / 12  # 3 th√°ng = 0.25, 6 th√°ng = 0.5, 12 th√°ng = 1.0

        # T√≠nh 14 ch·ªâ s·ªë sau shock
        indicators_after = excel_processor.simulate_scenario_full_propagation(
            original_indicators=indicators,
            revenue_change_pct=micro_shocks['revenue_change_pct'] * time_multiplier,
            interest_rate_change_pct=micro_shocks['interest_rate_change_pct'] * time_multiplier,
            cogs_change_pct=micro_shocks['cogs_change_pct'] * time_multiplier,
            liquidity_shock_pct=micro_shocks['liquidity_shock_pct'] * time_multiplier
        )

        # D·ª± b√°o PD
        feature_cols = [f'X_{i}' for i in range(1, 15)]
        X_future = np.array([[indicators_after[col] for col in feature_cols]])

        pd_future = self.stacking_model.predict_proba(X_future)[0, 1] * 100

        return round(pd_future, 2)

    def generate_gemini_diagnosis(
        self,
        health_score: float,
        risk_info: Dict[str, str],
        weaknesses: List[Dict[str, Any]],
        cluster_info: Dict[str, Any],
        pd_projections: Dict[str, Any],
        current_pd: float,
        gemini_api_key: Optional[str] = None
    ) -> str:
        """
        T·∫°o b√°o c√°o ch·∫©n ƒëo√°n b·∫±ng Gemini AI

        Args:
            health_score: Health Score
            risk_info: Th√¥ng tin risk level
            weaknesses: Top 3 ƒëi·ªÉm y·∫øu
            cluster_info: Th√¥ng tin cluster
            pd_projections: D·ª± b√°o PD t∆∞∆°ng lai
            current_pd: PD hi·ªán t·∫°i
            gemini_api_key: Gemini API key

        Returns:
            B√°o c√°o ch·∫©n ƒëo√°n (ti·∫øng Vi·ªát)
        """
        # L·∫•y Gemini API key t·ª´ environment n·∫øu kh√¥ng ƒë∆∞·ª£c truy·ªÅn v√†o
        if gemini_api_key is None:
            gemini_api_key = os.getenv('GEMINI_API_KEY')

        if not gemini_api_key:
            return self._generate_fallback_diagnosis(
                health_score, risk_info, weaknesses, cluster_info, pd_projections, current_pd
            )

        try:
            import google.generativeai as genai

            # Configure Gemini
            genai.configure(api_key=gemini_api_key)
            model = genai.GenerativeModel('gemini-2.0-flash')

            # T·∫°o prompt
            prompt = f"""
B·∫°n l√† chuy√™n gia ph√¢n t√≠ch t√≠n d·ª•ng c·ªßa Agribank. H√£y vi·∫øt b√°o c√°o ch·∫©n ƒëo√°n s·ª©c kh·ªèe t√†i ch√≠nh cho doanh nghi·ªáp n√†y.

**TH√îNG TIN CH·∫®N ƒêO√ÅN:**

1. **Health Score:** {health_score:.2f}/100
2. **M·ª©c r·ªßi ro:** {risk_info['risk_level_icon']} {risk_info['risk_level_text']}
3. **PD hi·ªán t·∫°i:** {current_pd:.2f}%
4. **V·ªã tr√≠:** {cluster_info['cluster_name']} (X·∫øp h·∫°ng {cluster_info['position_percentile']:.1f}% trong 1300 DN)

**TOP 3 ƒêI·ªÇM Y·∫æU:**
{chr(10).join([f"- **{w['name']}**: Gi√° tr·ªã hi·ªán t·∫°i {w['current_value']:.2f}, ng∆∞·ª°ng an to√†n {w['safe_threshold']:.2f} (Gap: {w['gap']:.2f}, M·ª©c ƒë·ªô: {w['severity']})" for w in weaknesses])}

**D·ª∞ B√ÅO PD T∆Ø∆†NG LAI:**
- **3 th√°ng:**
  - Suy tho√°i nh·∫π: {pd_projections.get('recession_mild', {}).get('3_months', 0):.2f}%
  - Suy tho√°i trung b√¨nh: {pd_projections.get('recession_moderate', {}).get('3_months', 0):.2f}%
  - Kh·ªßng ho·∫£ng: {pd_projections.get('crisis', {}).get('3_months', 0):.2f}%

- **6 th√°ng:**
  - Suy tho√°i nh·∫π: {pd_projections.get('recession_mild', {}).get('6_months', 0):.2f}%
  - Suy tho√°i trung b√¨nh: {pd_projections.get('recession_moderate', {}).get('6_months', 0):.2f}%
  - Kh·ªßng ho·∫£ng: {pd_projections.get('crisis', {}).get('6_months', 0):.2f}%

- **12 th√°ng:**
  - Suy tho√°i nh·∫π: {pd_projections.get('recession_mild', {}).get('12_months', 0):.2f}%
  - Suy tho√°i trung b√¨nh: {pd_projections.get('recession_moderate', {}).get('12_months', 0):.2f}%
  - Kh·ªßng ho·∫£ng: {pd_projections.get('crisis', {}).get('12_months', 0):.2f}%

**Y√äU C·∫¶U:**
H√£y vi·∫øt b√°o c√°o ch·∫©n ƒëo√°n v·ªõi c·∫•u tr√∫c sau (s·ª≠ d·ª•ng Markdown):

## üìã CH·∫®N ƒêO√ÅN T·ªîNG QUAN
(2-3 c√¢u t√≥m t·∫Øt t√¨nh h√¨nh s·ª©c kh·ªèe t√†i ch√≠nh v√† m·ª©c ƒë·ªô r·ªßi ro)

## üîç PH√ÇN T√çCH CHI TI·∫æT

### ƒêi·ªÉm m·∫°nh
(Li·ªát k√™ 2-3 ƒëi·ªÉm m·∫°nh c·ªßa DN)

### ƒêi·ªÉm y·∫øu c·∫ßn c·∫£i thi·ªán
(Ph√¢n t√≠ch chi ti·∫øt 3 ƒëi·ªÉm y·∫øu ƒë∆∞·ª£c li·ªát k√™ ·ªü tr√™n, gi·∫£i th√≠ch t·∫°i sao ch√∫ng quan tr·ªçng v√† ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o)

## üí° KHUY·∫æN NGH·ªä

### Ng·∫Øn h·∫°n (0-3 th√°ng)
(2-3 khuy·∫øn ngh·ªã c·ª• th·ªÉ)

### Trung h·∫°n (3-12 th√°ng)
(2-3 khuy·∫øn ngh·ªã c·ª• th·ªÉ)

## ‚ö†Ô∏è ƒê√ÅNH GI√Å R·ª¶I RO

### Kh·∫£ nƒÉng ch·ªëng ch·ªãu v·ªõi suy tho√°i kinh t·∫ø
(Ph√¢n t√≠ch d·ª±a tr√™n d·ª± b√°o PD t∆∞∆°ng lai)

### Quy·∫øt ƒë·ªãnh t√≠n d·ª•ng
(Khuy·∫øn ngh·ªã: Ch·∫•p thu·∫≠n / C√¢n nh·∫Øc / T·ª´ ch·ªëi - k√®m ƒëi·ªÅu ki·ªán c·ª• th·ªÉ)

---
**L∆∞u √Ω:** Vi·∫øt ng·∫Øn g·ªçn, chuy√™n nghi·ªáp, d·ªÖ hi·ªÉu. Tr√°nh l·∫∑p l·∫°i th√¥ng tin. T·∫≠p trung v√†o insights v√† actionable recommendations.
"""

            # G·ªçi Gemini API
            response = model.generate_content(prompt)
            diagnosis = response.text

            return diagnosis

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi g·ªçi Gemini API: {str(e)}")
            return self._generate_fallback_diagnosis(
                health_score, risk_info, weaknesses, cluster_info, pd_projections, current_pd
            )

    def _generate_fallback_diagnosis(
        self,
        health_score: float,
        risk_info: Dict[str, str],
        weaknesses: List[Dict[str, Any]],
        cluster_info: Dict[str, Any],
        pd_projections: Dict[str, Any],
        current_pd: float
    ) -> str:
        """T·∫°o b√°o c√°o ch·∫©n ƒëo√°n fallback (kh√¥ng d√πng Gemini)"""

        diagnosis = f"""## üìã CH·∫®N ƒêO√ÅN T·ªîNG QUAN

Doanh nghi·ªáp c√≥ **Health Score {health_score:.2f}/100**, thu·ªôc m·ª©c **{risk_info['risk_level_icon']} {risk_info['risk_level_text']}** v·ªõi PD hi·ªán t·∫°i **{current_pd:.2f}%**.

V·ªã tr√≠: **{cluster_info['cluster_name']}** (x·∫øp h·∫°ng **{cluster_info['position_percentile']:.1f}%** trong 1300 DN).

## üîç PH√ÇN T√çCH CHI TI·∫æT

### ƒêi·ªÉm y·∫øu c·∫ßn c·∫£i thi·ªán

"""

        # Li·ªát k√™ 3 ƒëi·ªÉm y·∫øu
        for i, w in enumerate(weaknesses, 1):
            diagnosis += f"""{i}. **{w['name']}**
   - Gi√° tr·ªã hi·ªán t·∫°i: {w['current_value']:.2f}
   - Ng∆∞·ª°ng an to√†n: {w['safe_threshold']:.2f}
   - Kho·∫£ng c√°ch: {w['gap']:.2f} ({w['severity']})
   - Percentile: {w['percentile']:.1f}%

"""

        diagnosis += f"""## üí° KHUY·∫æN NGH·ªä

### Ng·∫Øn h·∫°n (0-3 th√°ng)
- T·∫≠p trung c·∫£i thi·ªán c√°c ch·ªâ s·ªë y·∫øu nh·∫•t (ƒë·∫∑c bi·ªát l√† {weaknesses[0]['name']})
- TƒÉng c∆∞·ªùng thanh kho·∫£n v√† qu·∫£n l√Ω d√≤ng ti·ªÅn
- Xem x√©t t√°i c∆° c·∫•u n·ª£ n·∫øu c·∫ßn thi·∫øt

### Trung h·∫°n (3-12 th√°ng)
- C·∫£i thi·ªán hi·ªáu qu·∫£ ho·∫°t ƒë·ªông kinh doanh
- T·ªëi ∆∞u h√≥a c·∫•u tr√∫c v·ªën
- ƒêa d·∫°ng h√≥a ngu·ªìn thu

## ‚ö†Ô∏è ƒê√ÅNH GI√Å R·ª¶I RO

### Kh·∫£ nƒÉng ch·ªëng ch·ªãu v·ªõi suy tho√°i kinh t·∫ø

D·ª±a tr√™n m√¥ ph·ªèng:
- **Suy tho√°i nh·∫π (12 th√°ng):** PD tƒÉng l√™n {pd_projections.get('recession_mild', {}).get('12_months', 0):.2f}%
- **Suy tho√°i trung b√¨nh (12 th√°ng):** PD tƒÉng l√™n {pd_projections.get('recession_moderate', {}).get('12_months', 0):.2f}%
- **Kh·ªßng ho·∫£ng (12 th√°ng):** PD tƒÉng l√™n {pd_projections.get('crisis', {}).get('12_months', 0):.2f}%

### Quy·∫øt ƒë·ªãnh t√≠n d·ª•ng

"""

        if health_score >= 70:
            diagnosis += "**‚úÖ Khuy·∫øn ngh·ªã: Ch·∫•p thu·∫≠n** - Doanh nghi·ªáp c√≥ s·ª©c kh·ªèe t√†i ch√≠nh t·ªët."
        elif health_score >= 50:
            diagnosis += "**‚ö†Ô∏è Khuy·∫øn ngh·ªã: C√¢n nh·∫Øc** - Y√™u c·∫ßu th√™m t√†i s·∫£n ƒë·∫£m b·∫£o ho·∫∑c ƒëi·ªÅu ki·ªán b·ªï sung."
        else:
            diagnosis += "**‚ùå Khuy·∫øn ngh·ªã: T·ª´ ch·ªëi ho·∫∑c y√™u c·∫ßu c·∫£i thi·ªán** - R·ªßi ro cao, c·∫ßn c·∫£i thi·ªán s·ª©c kh·ªèe t√†i ch√≠nh tr∆∞·ªõc khi xem x√©t."

        return diagnosis


# Kh·ªüi t·∫°o instance global
early_warning_system = EarlyWarningSystem()
